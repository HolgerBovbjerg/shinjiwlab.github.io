---
layout: distill
title: Foundations for Speech Foundation Models
description: A summary of our recent work at WAVLab towards building large-scale speech foundation models
date: 2023-09-24
giscus_comments: true

authors:
  - name: William Chen
    url: "https://wanchichen.github.io/"
    affiliations:
      name: Carnegie Mellon University
  - name: Jiatong Shi
    url: "http://shijt.site"
    affiliations:
        name: Carnegie Mellon University
  - name: Shinji Watanabe
    url: "https://sites.google.com/view/shinjiwatanabe"
    affiliations:
      name: Carnegie Mellon University

bibliography: 2023-09-24-foundations.bib

toc:
  - name: "YODAS: 420k Hours of Annotated Multilingual Data"
  - name: "OWSM: Understanding Large-scale Weak Supervision"
  - name: "ML-SUPERB: Community-Driven Benchmarking for over 150 Languages"
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Introduction 
The explosion in generative AI has taken the world by storm: powerful pretrained models like GPT-4 and Stable Diffusion have already entered the mainstream media and consumer pockets. While the trend towards large-scale models is no different in speech, a concensus has yet to be seen on what techniques will drive the speech foundation models of tomorrow. To help enable this progress, we are very excited to share the techniques and resources we have been developing at WAVLab, many of which will be publicly released for both academic and commerical use in the coming weeks.

If you're reading this in 2023, many of the discussed works will be presented at [ASRU](http://www.asru2023.org/). Check out our presentations in Taipei if you want more details about YODAS, the ML-SUPERB Challenge, OWSM, and WAVLabLM.

## YODAS: 420k Hours of Annotated Multilingual Data
{% details Authors %}
*Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, Shinji Watanabe*
{% enddetails %}
&nbsp;

Unlike text-driven Large Language Models, many spoken language tasks are inherently multi-modal: we often interact with these speech models through text, either as an input or output. This makes paired speech-text data a neccessity, but it is much more difficult to acquire compared to unpaired speech or unpaired text. Companies like Google and Meta are able to train large-scale speech foundation models <d-cite key="pmlr-v202-radford23a, zhang2023google,barrault2023seamlessm4t,pratap2023scaling"></d-cite> through their access to considerable amounts of internal paired data that remain unreleased, often due to privacy or copyright restrictions. **How can researchers train more powerful models using the newest techniques, without access to sufficient amounts of data?**

Our answer is YODAS, a Youtube-Oriented Dataset for Audio and Speech that consists of **over 500k hours of speech data across 140 languages, with 420k hours of the data having paired textual transcripts**. To create YODAS, we extensively crawled Youtube for about half a year, collecting both audio data and the provided transcriptions. These transcriptions however, are not synced with the speech. We need to first align each sentence in the transcript to timestamps in the audio, after which we can segment the audio into smaller clips. Without this step, the audio would be too long and not fit into the GPU for model training (we address this in a future work coming soon!).

To perform the segmentation, we used a pre-trained acoustic model <d-cite key="liUniversal"></d-cite> to align the speech and text. Along with the speech-text alignments, the model also gives us a score based off the CTC loss. We can thus use this as a metric to determine the quality of the aligned speech/text and filter out poor quality samples. A per-language and per-writing system breakdown of the filtered dataset are shown below:

{% include figure.html path="assets/img/blog/yodas_langs.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
Total duration (measured in hours) of the dataset for the top 25 languages. Manual subtitles (blue) are uploaded by the user, while automatic subtitles are generated by Youtube (orange).
</div>

{% include figure.html path="assets/img/blog/yodas_chars.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
Number of occurences of each character type in the YODAS transcripts, on a log-scale.
</div>

**Most importantly, we only crawled videos released with a Creative Commons 3.0 License, meaning all of our data can be made open-source and even used commerically!** We plan to release the data over HuggingFace Datasets in the next months, so stay tuned! If you're interested in more details about our crawling method or data distribution, the paper will also be released on arXiv soon.

## OWSM: Understanding Large-scale Weak Supervision

{% details Authors %}
*Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe*
{% enddetails %}
&nbsp;

While the attention of speech researchers has been mostly occupied by self-supervised BERT-style models in the past several years, the introduction of Whisper <d-cite key="pmlr-v202-radford23a"></d-cite> has significantly strengthened the case for semi-supervised / weakly-supervised models. Whisper is trained on an extremely large scale collection of paired speech/text data, sacrificing data quality for quantity. This leads to very impressive zero-shot performance on new domains and tasks, such as unseen speech translation pairs and code-switched ASR <d-cite key="peng23d_interspeech"></d-cite>.

But using such large-scale proprietary models for research is risky. As the scale of AI models grow, the chance of data corruption only gets higher. **How can researchers understand the capabilites of these models without knowing the data they are trained on?** Our goal is to produce a model with the capabilities of Whisper, but with full transparency on the training data. We are excited to share our first steps towards this direction: OWSM (Open Whisper-style Speech Model).

Similar to Whisper, OWSM is a Transformer encoder-decoder trained on 30 second segments of paired speech/text data. The model is trained to perform multiple tasks, such as ASR, language identification, speech translation, and timestamp prediction. However, there are also a few key differences. OWSM downsamples the input by 4 times instead of 2 times, for better training efficiency. We also employ an auxilliary CTC loss, which stabilizes training. It also allows OWSM to perform joint CTC/attention decoding, which helps prevents repeated tokens and makes inference parameters easier to tune. Finally, OWSM supports any-to-any speech translation, while Whisper can only perform any-to-English.

{% include figure.html path="assets/img/blog/owsm_pipeline.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
Training pipeline of OWSM.
</div>

OWSM is trained exclusively on publicly accessible datasets, which totals to over 180k hours of speech, roughly a quarter to that of Whisper's 680k. **This makes OWSM by far the largest speech model trained by an academic group, and rivals many proposed by industrial research labs.** Training the final version of OWSM took 10 days on 64 A100 40GB GPUs, or about 15,400 GPU hours. Counting our abalations and scaling experiments, we consumed around 36,000 total GPU hours, or about half of our computational budget for the whole year! The complete details will be included in the arxiv edition of our paper. **We will be working to scale OWSM to 1 million hours of data. So if you want collaborate/sponsor the next generation of spoken language models, don't hesitate to reach out!**


## WAVLabLM: Multilingual Self-Supervised Speech Representations
{% details Authors %}
*William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe*
{% enddetails %}
&nbsp;

## ML-SUPERB: Community-Driven Benchmarking for over 150 Languages
{% details Authors %}
*Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe*
{% enddetails %}
&nbsp;


## VoxtLM: Multi-task Spoken Language Modelling

