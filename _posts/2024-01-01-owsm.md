---
layout: post
title: Open Whisper-style Speech Models (OWSM)
description: This is the project page for OWSM models.
date: 2024-01-01 00:00:00-0800
comments: false
---

## Overview

**O**pen **W**hisper-style **S**peech **M**odels (OWSM, pronounced as "awesome") are a series of speech foundation models developed by [WAVLab](https://www.wavlab.org/) at Carnegie Mellon University. We reproduce Whisper-style training using publicly available data and our open-source toolkit [ESPnet](https://github.com/espnet/espnet). By publicly releasing data preparation scripts, training and inference code, pre-trained model weights and training logs, we aim to promote transparency and open science in large-scale speech pre-training.

## Demo

- Gradio demo: [![Static Badge](https://img.shields.io/badge/OWSM-Demo-orange)](https://pyf98-owsm-v3-demo.hf.space)
- Colab notebook: [![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1zKI3ZY_OtZd6YmVeED6Cxy1QwT1mqv9O?usp=sharing)

## Pre-trained models

We publicly release a series of pre-trained models. The training logs are also available for major models. We recommend using OWSM v3.1 or later for better performance and efficiency.

<table class="table">
    <thead>
      <tr>
        <th>Name</th>
        <th>Data (hours)</th>
        <th>Encoder</th>
        <th>Parameters</th>
        <th>Model Link</th>
        <th>ESPnet Recipe</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>OWSM v1</td>
        <td>38k</td>
        <td>Transformer</td>
        <td>272M</td>
        <td><a href="https://huggingface.co/espnet/owsm_v1">espnet/owsm_v1</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v1/s2t1">egs2/owsm_v1/s2t1</a></td>
      </tr>
      <tr>
        <td>OWSM v2</td>
        <td>129k</td>
        <td>Transformer</td>
        <td>712M</td>
        <td><a href="https://huggingface.co/espnet/owsm_v2">espnet/owsm_v2</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v2/s2t1">egs2/owsm_v2/s2t1</a></td>
      </tr>
      <tr>
        <td>OWSM v2</td>
        <td>129k</td>
        <td>E-Branchformer</td>
        <td>739M</td>
        <td><a href="https://huggingface.co/espnet/owsm_v2_ebranchformer">espnet/owsm_v2_ebranchformer</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v2/s2t1">egs2/owsm_v2/s2t1</a></td>
      </tr>
      <tr>
        <td>OWSM v3</td>
        <td>180k</td>
        <td>Transformer</td>
        <td>889M</td>
        <td><a href="https://huggingface.co/espnet/owsm_v3">espnet/owsm_v3</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v3/s2t1">egs2/owsm_v3/s2t1</a></td>
      </tr>
      <tr>
        <td><b>OWSM v3.1 base</b></td>
        <td>180k</td>
        <td>E-Branchformer</td>
        <td>101M</td>
        <td><a href="https://huggingface.co/espnet/owsm_v3.1_ebf_base">espnet/owsm_v3.1_ebf_base</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v3.1/s2t1">egs2/owsm_v3.1/s2t1</a></td>
      </tr>
      <tr>
        <td><b>OWSM v3.1 small</b></td>
        <td>180k</td>
        <td>E-Branchformer</td>
        <td>367M</td>
        <td><a href="">Coming soon</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v3.1/s2t1">egs2/owsm_v3.1/s2t1</a></td>
      </tr>
      <tr>
        <td><b>OWSM v3.1 medium</b></td>
        <td>180k</td>
        <td>E-Branchformer</td>
        <td>1.02B</td>
        <td><a href="https://huggingface.co/espnet/owsm_v3.1_ebf">espnet/owsm_v3.1_ebf</a></td>
        <td><a href="https://github.com/espnet/espnet/tree/master/egs2/owsm_v3.1/s2t1">egs2/owsm_v3.1/s2t1</a></td>
      </tr>
      <tr>
        <td><b>OWSM v3.1 medium license-free</b></td>
        <td>70k</td>
        <td>E-Branchformer</td>
        <td>1.02B</td>
        <td><a href="">Coming soon</a></td>
        <td><a href="">Coming soon</a></td>
      </tr>
    </tbody>
</table>

## Data details



## Inference

### Language Identification

### Speech Recognition

### Speech Translation

### Long-form Speech Recognition or Translation


## Fine-tuning

Coming soon!


## Papers

### OWSM core papers

Please cite our papers if you use OWSM in your project.

- ASRU 2023: [Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data](https://arxiv.org/abs/2309.13876)

### Other related papers

Here is a list of papers related to OWSM. Please contact Yifan Peng (yifanpen@andrew.cmu.edu) if you use OWSM in your work and want to include it here.

#### Applications

<details>
<ul>
  <li>Coming soon</li>
</ul>
</details>
<br>

#### Model architecture

<details>
<ul>
  <li>INTERSPEECH 2023: <a href="https://arxiv.org/abs/2305.11073">A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks</a></li>
  <li>SLT 2022: <a href="https://proceedings.mlr.press/v162/peng22a.html">E-Branchformer: Branchformer with Enhanced merging for speech recognition</a></li>
  <li>ICML 2022: <a href="https://proceedings.mlr.press/v162/peng22a.html">Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding</a></li>
</ul>
</details>
<br>
